# Minor
ML based project for image captioning
This project presents a model that generates captions or descriptions for images with the help of multimodal neural networks. The model consists of two subnetworks a convolution neural network that is utilized to extract the image characteristics and a recurrent neural network for the descriptions. These sub-networks are then aligned through a multimodal embedding to form the whole model. .As compared to previous models such as temporal convolution, the recurrent models are considered to be doubly deep. These recurrent neural networks can handle variable input/output. Thus, they can directly map a variable input (e.g. video) with a variable output (e.g. a caption/description, natural language text). The results distinctly show its advantage over state-of-the-art models that are used for generating descriptions or captioning images.
